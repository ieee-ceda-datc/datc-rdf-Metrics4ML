{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Users/ravivaradarajan/opt/anaconda3/lib/python3.8/site-packages (1.19.2)\n",
      "Requirement already satisfied: pandas in /Users/ravivaradarajan/opt/anaconda3/lib/python3.8/site-packages (1.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/ravivaradarajan/opt/anaconda3/lib/python3.8/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /Users/ravivaradarajan/opt/anaconda3/lib/python3.8/site-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /Users/ravivaradarajan/opt/anaconda3/lib/python3.8/site-packages (from pandas) (1.19.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/ravivaradarajan/opt/anaconda3/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Requirement already satisfied: seaborn in /Users/ravivaradarajan/opt/anaconda3/lib/python3.8/site-packages (0.11.0)\n",
      "Requirement already satisfied: pandas>=0.23 in /Users/ravivaradarajan/opt/anaconda3/lib/python3.8/site-packages (from seaborn) (1.1.3)\n",
      "Requirement already satisfied: scipy>=1.0 in /Users/ravivaradarajan/opt/anaconda3/lib/python3.8/site-packages (from seaborn) (1.5.2)\n",
      "Requirement already satisfied: matplotlib>=2.2 in /Users/ravivaradarajan/opt/anaconda3/lib/python3.8/site-packages (from seaborn) (3.3.2)\n",
      "Requirement already satisfied: numpy>=1.15 in /Users/ravivaradarajan/opt/anaconda3/lib/python3.8/site-packages (from seaborn) (1.19.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /Users/ravivaradarajan/opt/anaconda3/lib/python3.8/site-packages (from pandas>=0.23->seaborn) (2020.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/ravivaradarajan/opt/anaconda3/lib/python3.8/site-packages (from pandas>=0.23->seaborn) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /Users/ravivaradarajan/opt/anaconda3/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (2.4.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/ravivaradarajan/opt/anaconda3/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (8.0.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/ravivaradarajan/opt/anaconda3/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/ravivaradarajan/opt/anaconda3/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (1.3.0)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in /Users/ravivaradarajan/opt/anaconda3/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (2020.6.20)\n",
      "Requirement already satisfied: six>=1.5 in /Users/ravivaradarajan/opt/anaconda3/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas>=0.23->seaborn) (1.15.0)\n",
      "Requirement already satisfied: plotly in /Users/ravivaradarajan/opt/anaconda3/lib/python3.8/site-packages (4.14.3)\n",
      "Requirement already satisfied: six in /Users/ravivaradarajan/opt/anaconda3/lib/python3.8/site-packages (from plotly) (1.15.0)\n",
      "Requirement already satisfied: retrying>=1.3.3 in /Users/ravivaradarajan/opt/anaconda3/lib/python3.8/site-packages (from plotly) (1.3.3)\n",
      "Requirement already satisfied: scikit-learn in /Users/ravivaradarajan/opt/anaconda3/lib/python3.8/site-packages (0.23.2)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /Users/ravivaradarajan/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/ravivaradarajan/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn) (0.17.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/ravivaradarajan/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn) (2.1.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /Users/ravivaradarajan/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn) (1.19.2)\n"
     ]
    }
   ],
   "source": [
    "#Install require packages\n",
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install seaborn\n",
    "!pip install plotly\n",
    "!pip install scikit-learn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob, os\n",
    "\n",
    "from datetime import date, timedelta\n",
    "\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design :  ibex <br />  <br /> Platform: sky130hd\n",
    "\n",
    "A sample dataset with multiple runs varying the design utilization and the layer_adjust parameters for the routing layers. All design metrics from the runs are collected for analysis.\n",
    "\n",
    "Metrics data is represented as json files in the  METRICS2.1 format. Each experiment in the run is a separate json file. All of the files are read into a DataFrame 'json_df'.\n",
    "\n",
    "* Rows of the DataFrame represent an experiment.\n",
    "* Columns of the DataFrame represent the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './metrics'\n",
    "json_df = pd.DataFrame()\n",
    "for filename in glob.glob(os.path.join(path, '*.json')):\n",
    "    with open(os.path.join(os.getcwd(), filename), 'r') as f:\n",
    "        data = f.read()\n",
    "    data_json = json_normalize(json.loads(data))\n",
    "    json_df = json_df.append(data_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of runs in the dataset: {json_df.shape[0]}')\n",
    "print(f'Number of metrics in each data: {json_df.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The columns in the DataFrame, which are the metrics colleced for each run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Engineering**\n",
    "* Extract only the relevant columns -- for this exercise, get the design name, flow variant and route metrics.\n",
    "* Parse Variant and extract the relevant features  -- Utilization, layer adjust for each routing layer.\n",
    "* Rename columns.\n",
    "* Create new columns (features) based on other column values -- for e.g. Success/Fail for the router."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Engineering, extract the relevant metrics\n",
    "metrics_df = json_df[['run.flow__design',\n",
    "                      'run.flow__variant',\n",
    "                      'globalroute.timing__setup__ws',\n",
    "                      'detailedroute.route__wirelength',\n",
    "                      'detailedroute.route__via__count',\n",
    "                      'detailedroute.route__drc_errors__count',\n",
    "                      'detailedroute.runtime__total'\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = metrics_df.rename({\n",
    "    'run.flow__design' : 'Design',\n",
    "    'run.flow__variant' : 'Variant',\n",
    "    'globalroute.timing__setup__ws' : 'GR WNS',\n",
    "    'detailedroute.route__wirelength' : 'Wire Length',\n",
    "    'detailedroute.route__via__count' : 'Vias',\n",
    "    'detailedroute.route__drc_errors__count' : 'DRC Errors',\n",
    "    'detailedroute.runtime__total' : 'Route Runtime'\n",
    "}, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Print the head of the head of the data frame. As we can see, all of the parameter values used in the experiment is in the \"Variant\" string.  We will have to parse the string and create individual columns for the features we are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df['GR Success'] = metrics_df['Success'] = True\n",
    "metrics_df.loc[metrics_df['GR WNS'] == 'N/A','GR Success'] = False\n",
    "metrics_df.loc[metrics_df['GR WNS'] == 'ERR','GR Success'] = False\n",
    "\n",
    "metrics_df.loc[metrics_df['DRC Errors'] == 'ERR','Success'] = False\n",
    "metrics_df.loc[metrics_df['Wire Length'] == 'N/A','Success'] = False\n",
    "metrics_df.loc[metrics_df['Wire Length'] == 'ERR','Success'] = False\n",
    "metrics_df.loc[metrics_df['Route Runtime'] == 'N/A','Success'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metrics_df[(metrics_df['GR Success'] == True) & (metrics_df['Success'] == False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metrics_df[(metrics_df['Route Runtime'] == 'N/A') & (metrics_df['Success'] == True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_variant(val):\n",
    "    variants = val.split(\"-\")\n",
    "    v_dict = dict()\n",
    "    for i in range(len(variants)):\n",
    "        key_val = variants[i].rsplit('_',1)\n",
    "        v_dict[key_val[0]] = float(key_val[1])\n",
    "    return pd.Series([v_dict['CORE_UTIL'], v_dict['M1'], v_dict['M2'], v_dict['M3'], v_dict['M4'], v_dict['M5']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df[['Core Util', 'M1 Adjust', 'M2 Adjust', 'M3 Adjust', 'M4 Adjust', 'M5 Adjust']] = metrics_df['Variant'].apply(parse_variant)\n",
    "metrics_df['Weighted Adjust'] = (metrics_df['M1 Adjust'] + metrics_df['M2 Adjust'] + metrics_df['M3 Adjust'] +\n",
    "                                 metrics_df['M4 Adjust'] + metrics_df['M5 Adjust']) / 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Extract individual features from the \"Variant\" string.\n",
    "* Print the head of the dataframe after extracting the features and performing further data engineering.  This data frame is now ready to be used.  Notice that we have also created additional columns for capturing whether Global Route and Detailed Route completed. We have also caluclated a 'Weighted Adjust' column that is a simple mean of the 'layer adjust' for each of the routing layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success_df = metrics_df[metrics_df['Success'] == True]\n",
    "success_df = success_df.astype({'Design' : 'string',\n",
    "                               'Variant' : 'string',\n",
    "                                'GR WNS' : 'float',\n",
    "                                'Vias'   : 'int',\n",
    "                                'DRC Errors' : 'int',\n",
    "                                'Wire Length' : 'float',\n",
    "                                'Route Runtime' : 'string'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_runtime_str(rstr):\n",
    "    rstr = rstr.split('.')[0]\n",
    "    hms = rstr.split(':')\n",
    "    if len(hms) == 3:\n",
    "        runtime = int(hms[0]) * 3600 + int(hms[1]) * 60 + int(hms[2])\n",
    "    elif len(hms) == 2:\n",
    "        runtime = int(hms[0]) * 60 + int(hms[1])\n",
    "    elif len(hms) == 1:\n",
    "        runtime = int(hms[0])\n",
    "    return runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success_df['Runtime'] = success_df['Route Runtime'].apply(convert_runtime_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Print the statistics for each of the entries in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success_df.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can observe a huge variation in runtime for the different runs and also a substantial variation in the routed wirelength. We would like to observe the relation between the various parameters on both runtime and routed wirelength."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print some of base metrics from the data set.\n",
    "* Number of success/failures.\n",
    "* Min, Max of Wirelength, number of vias, drc errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metrics_df['Success'].value_counts()\n",
    "print(f'Number of successful runs: {metrics_df[metrics_df[\"Success\"]  == 1].shape[0]}')\n",
    "print(f'Number of failed runs: {metrics_df[metrics_df[\"Success\"]  == 0].shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_wire_length = success_df['Wire Length'].min()\n",
    "max_wire_length = success_df['Wire Length'].max()\n",
    "print(f\"Min Wire Length: {min_wire_length},  Max Wire Length: {max_wire_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_vias = success_df['Vias'].min()\n",
    "max_vias = success_df['Vias'].max()\n",
    "print(f\"Min Vias: {min_vias},  Max Vias: {max_vias}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_drc_errors = success_df['DRC Errors'].min()\n",
    "max_drc_errors = success_df['DRC Errors'].max()\n",
    "print(f\"Min DRC Errors: {min_drc_errors},  Max DRC Errors: {max_drc_errors}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_df = metrics_df[metrics_df['Success'] == 1].groupby(['Core Util'], as_index = False)['Success'].count()\n",
    "f_df = metrics_df[metrics_df['Success'] == 0].groupby(['Core Util'], as_index = False)['Success'].count()\n",
    "#s_df.groupby('Core Util')['Success'].value_counts().plot(kind = \"bar\", stacked=True, figsize= (10,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Plot the number of successful runs and number of failures with respect to core utlization. We can see that the trends are what we would expect.  As utilization increases, the number of successful runs decreases and the number of failure runs increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig = make_subplots(rows=1, cols=2,\n",
    "                    subplot_titles=('Success', 'Failure'),\n",
    "                    shared_xaxes=True,\n",
    "                   horizontal_spacing=0.1)\n",
    "fig.add_trace(go.Bar(x=s_df['Core Util'], y = s_df['Success'], marker = dict(color=\"green\")),\n",
    "              row=1, col=1)\n",
    "fig.add_trace(go.Bar(x=f_df['Core Util'], y = f_df['Success'], marker = dict(color = \"red\")),\n",
    "              row=1, col=2)\n",
    "fig.update_layout(width=1000, height=400,\n",
    "                   title = 'Number of Success and Failures at different Utilizations', title_x = 0.3,\n",
    "                   margin = dict(l=5, r=50, b=60, t=80, pad=4),\n",
    "                   showlegend = False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can observe that at utilization values of above 38, there is a drastic increase in the number of failures. We also observe that for the range of layer_adjust settings that we are using, it does not have a direct impact on the number of successful and doomed runs.  Choosing higher layer adjust values will show a more direct correlation to success/doomed runs.\n",
    "<br />\n",
    "\n",
    "* Generate a scatter plot of Wirelength Vs Core Utilization.  We can see the trend where the wirelength decreases with increased utilization. However, at a certain utilization value the wirelength starts to increase due to the router trying more detours to resolve DRC errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(x = success_df['Core Util'], y = success_df['Wire Length'],\n",
    "                         hovertext = success_df['Weighted Adjust'],\n",
    "                         mode = 'markers'))\n",
    "\n",
    "fig.update_layout(width=1000, height=400,\n",
    "                   title = 'Wire Length Vs Core Utilization', title_x = 0.5,\n",
    "                   margin = dict(l=5, r=50, b=60, t=80, pad=4),\n",
    "                   showlegend = False)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let us now examine how the route runtime varies with utilization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(x = success_df['Core Util'], y = success_df['Runtime'],\n",
    "                         hovertext = success_df['Weighted Adjust'],\n",
    "                         mode = 'markers'))\n",
    "\n",
    "fig.update_layout(width=1000, height=400,\n",
    "                   title = 'Runtime Vs Core Utilization', title_x = 0.5,\n",
    "                   margin = dict(l=5, r=50, b=60, t=80, pad=4),\n",
    "                   showlegend = False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can see that the runtime is fairly flat for smaller utilizations. As the utilization value goes above 35 the runime starts to degrade and rise exponentially.\n",
    "\n",
    "* Let's now examine the number of DRC errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(x = success_df['Core Util'], y = success_df['DRC Errors'],\n",
    "                         hovertext = success_df['Runtime'],\n",
    "                         mode = 'markers'))\n",
    "\n",
    "fig.update_layout(width=1000, height=400,\n",
    "                   title = 'DRC Errors Vs Core Utilization', title_x = 0.5,\n",
    "                   margin = dict(l=5, r=50, b=60, t=80, pad=4),\n",
    "                   showlegend = False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Once again, we see an expected trend -- the number of DRC errors is 0 for lower utilizations and starts to increase with utilizations above 35. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression\n",
    "\n",
    "* Now let use build a logistic regression model to predict successful or dommed runs for this design based on input parameters of utilization and layer adjust values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "X = metrics_df[['Core Util']]\n",
    "y = metrics_df['Success']\n",
    "\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(X, y)\n",
    "\n",
    "predicted_success = regr.predict([[20]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = ['Core Util', 'M1 Adjust', 'M2 Adjust', 'M3 Adjust', 'M4 Adjust', 'M5 Adjust']\n",
    "X = metrics_df[feature_cols]\n",
    "y = metrics_df['Success']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.20,random_state=0)\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = log_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Print the confusion matrix for the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Model Accuracy: {metrics.accuracy_score(y_test, y_pred):.4f}')\n",
    "print(f'Model Precision: {metrics.precision_score(y_test, y_pred):.4f}')\n",
    "print(f'Recall Score: {metrics.recall_score(y_test, y_pred):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The above model is very accurate. Now check to see the accuracy of the Weighted Adjust parameter as the sole predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [ 'Core Util', 'Weighted Adjust']\n",
    "X = metrics_df[feature_cols]\n",
    "y = metrics_df['Success']\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.20,random_state=0)\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = log_reg.predict(X_test)\n",
    "\n",
    "confusion_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "print(confusion_matrix)\n",
    "\n",
    "print(f'Model Accuracy: {metrics.accuracy_score(y_test, y_pred):.4f}')\n",
    "print(f'Model Precesion: {metrics.precision_score(y_test, y_pred):.4f}')\n",
    "print(f'Recall Score: {metrics.recall_score(y_test, y_pred):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For this  dataset, the weighted layer_adjust seem to have similar accuracy as the individual layer adjusts, as within the range of the layer adjusts, the core utilization has more direct impact on success or failure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now for runs that are successful,  let us predict the wirelength based on parameters using a simple linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#success_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "feature_cols = ['Core Util', 'M1 Adjust', 'M2 Adjust', 'M3 Adjust', 'M4 Adjust', 'M5 Adjust']\n",
    "X = success_df[feature_cols]\n",
    "y = success_df['Wire Length']\n",
    "\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(X, y)\n",
    "\n",
    "test_params = [34, 0.1, 0.1, 0.1, 0.1, 0.5]\n",
    "predicted_wire_length = regr.predict([test_params])\n",
    "\n",
    "actual_wire_length = success_df[((success_df['Core Util'] == 34) & (success_df['M1 Adjust'] == 0.1) & \n",
    "                                 (success_df['M2 Adjust'] == 0.1) &(success_df['M3 Adjust'] == 0.1) &\n",
    "                                 (success_df['M4 Adjust'] == 0.1) & (success_df['M5 Adjust'] == 0.5))].iloc[0]['Wire Length']\n",
    "\n",
    "\n",
    "print(f'Predicted Wire Length for input parameters: {test_params} is {predicted_wire_length[0]:.2f}')\n",
    "print(f'Actual Wire Length: {actual_wire_length}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to html --no-input --TemplateExporter.exclude_input=True --no-prompt METRICS2.1-fine-grain.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
